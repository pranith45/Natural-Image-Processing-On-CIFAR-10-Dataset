# -*- coding: utf-8 -*-
"""PRML_CourseProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NcZuVa7g5cesL0NJBvxmAtOBjs_jePzp

# **Without data augumentation**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix

from tensorflow.keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_train

fig = plt.figure(figsize=(10,5))
img = x_train[40]
plt.axis('off')
plt.imshow(img)
plt.show()

fig = plt.figure(figsize=(8, 8))
columns = 10
rows = 10
for i in range(1, columns*rows +1):
    img = x_train[i]
    fig.add_subplot(rows, columns, i)
    plt.axis('off')
    plt.imshow(img)
plt.show()

"""http://parneetk.github.io/blog/cnn-cifar10/"""

# class_names = ['airplane','automobile','bird','cat','deer',
#                'dog','frog','horse','ship','truck']
# fig = plt.figure(figsize=(8,3))
# num_classes = len(np.unique(y_train))
# for i in range(num_classes):
#     ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])
#     idx = np.where(y_train[:]==i)[0]
#     features_idx = x_train[idx,::]
#     img_num = np.random.randint(features_idx.shape[0])
#     im = np.transpose(features_idx[img_num,::], (1, 2, 0))
#     ax.set_title(class_names[i])
#     plt.imshow(im)
# plt.show()

print(x_train.max())
print(x_test.max())

# x_train = train.astype('float32')
# x_test = test.astype('float32')

x_train = x_train/225
x_test = x_test/255

y_cat_train = to_categorical(y_train, 10)
y_cat_test = to_categorical(y_test,10)

# model = Sequential()
# model.add(Convolution2D(48, 3, 3, border_mode='same', input_shape=(3, 32, 32)))
# model.add(Activation('relu'))
# model.add(Convolution2D(48, 3, 3))
# model.add(Activation('relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.25))
# model.add(Convolution2D(96, 3, 3, border_mode='same'))
# model.add(Activation('relu'))
# model.add(Convolution2D(96, 3, 3))
# model.add(Activation('relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.25))
# model.add(Convolution2D(192, 3, 3, border_mode='same'))
# model.add(Activation('relu'))
# model.add(Convolution2D(192, 3, 3))
# model.add(Activation('relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Dropout(0.25))
# model.add(Flatten())
# model.add(Dense(512))
# model.add(Activation('relu'))
# model.add(Dropout(0.5))
# model.add(Dense(256))
# model.add(Activation('relu'))
# model.add(Dropout(0.5))
# model.add(Dense(num_classes, activation='softmax'))



model = Sequential()

model.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Conv2D(filters=32, kernel_size=(4,4), activation='relu',))
model.add(MaxPool2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

early_stop = EarlyStopping(monitor='val_loss',patience=3)

# start = time.time()
# model_info = model.fit(train_features, train_labels, 
#                        batch_size=128, nb_epoch=200, 
#                        validation_data = (test_features, test_labels), 
#                        verbose=0)
# end = time.time()
# plot_model_history(model_info)
# print "Model took %0.2f seconds to train"%(end - start)
# print "Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model)

model.fit(x_train,y_cat_train,batch_size=128,epochs=20,validation_data=(x_test,y_cat_test),callbacks=[early_stop])

# from keras.preprocessing.image import ImageDataGenerator

# datagen = ImageDataGenerator(zoom_range=0.2,horizontal_flip=True)

# start = time.time()
# # Train the model
# model_info = model.fit_generator(datagen.flow(train_features, train_labels, batch_size = 128),
#                                  samples_per_epoch = train_features.shape[0], nb_epoch = 200, 
#                                  validation_data = (test_features, test_labels), verbose=0)
# end = time.time()
# print "Model took %0.2f seconds to train"%(end - start)
# # plot model history
# plot_model_history(model_info)
# # compute test accuracy
# print "Accuracy on test data is: %0.2f"%accuracy(test_features, test_labels, model)

losses = pd.DataFrame(model.history.history)

losses[['loss','val_loss']].plot()

losses[['accuracy','val_accuracy']].plot()

print(model.metrics_names)
print(model.evaluate(x_test,y_cat_test,verbose=0))

predictions = np.argmax(model.predict(x_test), axis=-1)

print(classification_report(y_test,predictions))

confusion_matrix(y_test,predictions)

classes = [0,1,2,3,4,5,6,7,8,9]
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
d = dict(zip(classes, class_names))

my_image = x_test[20]

plt.imshow(my_image)

input_img = my_image.reshape(1,32,32,3)

predictions = np.argmax(model.predict(input_img), axis=-1)[0]

print(f"True class: {d[y_test[20][0]]} \n\nPredicted class: {d[predictions]}")



# def unpickle(file):
#     import pickle
#     with open(file, 'rb') as fo:
#         dict = pickle.load(fo, encoding='bytes')
#     return dict

"""data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.

labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.

label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == "airplane", label_names[1] == "automobile", etc.

# **With data augumentation**

Dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""Importing Dataset"""

from tensorflow.keras.datasets import cifar10
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

y_train

"""Pre-Preocessing"""

X_train = np.asarray(X_train, dtype=float)/255
X_test = np.asarray(X_test, dtype=float)/255

X_test.shape

print("Number of train images: ", X_train.shape[0])
print("Number of test images: ", X_test.shape[0])

fig = plt.figure(figsize=(8, 8))
columns = 10
rows = 10
for i in range(1, columns*rows +1):
    img = X_train[i]
    fig.add_subplot(rows, columns, i)
    plt.axis('off')
    plt.imshow(img)
plt.show()

"""Build Model"""

from tensorflow import keras

# Model architecture
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import Sequential, datasets,models,layers
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D
from tensorflow.keras.layers import MaxPooling2D, DepthwiseConv2D, BatchNormalization
from tensorflow.keras.models import load_model

# Data processing
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

labels = []
for i in y_train:
  labels.append(i[0])
print(labels)

labels_df = pd.DataFrame(labels, columns=["labels"])
labels_df

classes = list(set(labels))
classes

number_of_classes = len(classes)
number_of_classes

labels_dict =  {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}
labels_dict_reversed = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}

labels_categories = keras.utils.to_categorical(labels_df["labels"], number_of_classes)

labels_categories

# Splitting the training data into train set and validation set
X_train, X_val, y_train, y_val = train_test_split(X_train, labels_categories, random_state=0, test_size=0.05)

print(X_train.shape)
print(X_val.shape)

"""Data Augmentation"""

# Data augumetation
datagen = ImageDataGenerator(
        rotation_range=0.3,  
        zoom_range = 0.1,  
        width_shift_range=0.1, 
        height_shift_range=0.1,
        horizontal_flip=True)

"""Model"""

model_layers = [
    Conv2D(32, (3, 3), activation='relu', strides=(1,1), padding='same', input_shape=(32, 32, 3)),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu, depth_multiplier=3),
#     MaxPooling2D(2, 2),
    Dropout(rate =0.1),
    
    
    Conv2D(64, (3, 3), activation='relu', strides=(2, 2), padding='same'),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),
#     MaxPooling2D(2, 2),
    Dropout(rate = 0.1),
    
    Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),
#     MaxPooling2D(2, 2),
    Dropout(rate = 0.4),
    
    Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(1,1), strides=(1, 1), padding='same', activation=keras.activations.relu),
    
    
    Conv2D(256, (3, 3), activation='relu', strides=(2, 2), padding='same'),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(3,3), strides=(1, 1), padding='same', activation=keras.activations.relu),
    
    
    
    Conv2D(512, (1, 1), activation='relu', strides=(2, 2), padding='same'),
    BatchNormalization(),
    DepthwiseConv2D(kernel_size=(1,1), strides=(1, 1), padding='same', activation=keras.activations.relu),
    
#     MaxPooling2D(2, 2),
    Dropout(rate = 0.4),
    
    Flatten(),
    Dropout(rate = 0.3),
    Dense(2048, activation='relu'),
    Dropout(rate = 0.3),
    Dense(512, activation='relu'),
    Dropout(rate = 0.4),
    Dense(10, activation='softmax')
] 
model = Sequential(model_layers)

model.summary()

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Visualization of the model
keras.utils.plot_model(model)

mcp_save = ModelCheckpoint('/content/best_model', save_best_only=True, monitor='val_accuracy', mode='max')

num_of_epochs = 10
batch_size = 64

model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), validation_data=(X_val, y_val), callbacks=[mcp_save], epochs=num_of_epochs)

print(model.metrics_names)
print(model.evaluate(X_val,y_val,verbose=0))

model_ = load_model('/content/best_model', compile=False)

test_predictions = model_.predict_classes(X_test)
test_predictions

accuracy = accuracy_score(test_predictions, y_test)
print(accuracy)

"""# **Random forest**

Dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""Importing Dataset"""

from tensorflow.keras.datasets import cifar10
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train=X_train.reshape(50000,3072)
X_test=X_test.reshape(10000,3072)
y_train = y_train.flatten()
y_test = y_test.flatten()
# y_train=y_train.reshape(50000,3072)
# y_test=y_test.reshape(10000,3072)

"""Pre-Preocessing"""

X_train = np.asarray(X_train, dtype=float)/255
X_test = np.asarray(X_test, dtype=float)/255
y_train = y_train.flatten()
y_test = y_test.flatten()

X_test.shape

print("Number of train images: ", X_train.shape[0])
print("Number of test images: ", X_test.shape[0])

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
# clf_rdf=RandomForestClassifier()
parameters = {'n_estimators':[10,50,100]}
clf_GS_rdf= GridSearchCV(RandomForestClassifier(), parameters)
clf_GS_rdf.fit(X_train,y_train)
clf_GS_rdf.best_params_

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score 
model = RandomForestClassifier(n_estimators = 100)
cv_results = cross_val_score(model, X_train, y_train,cv = 5, scoring='accuracy', n_jobs = -1, verbose = 1)  
model.fit(X_train, y_train)

from sklearn import metrics
print(cv_results.mean()*100)
predicted_y = model.predict(X_test)
print(metrics.classification_report(y_test, predicted_y))

"""# **MLP**

Dependencies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""Importing Dataset"""

from tensorflow.keras.datasets import cifar10
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train=X_train.reshape(50000,3072)
X_test=X_test.reshape(10000,3072)
y_train = y_train.flatten()
y_test = y_test.flatten()
# y_train=y_train.reshape(50000,3072)
# y_test=y_test.reshape(10000,3072)

"""Pre-Preocessing"""

X_train = np.asarray(X_train, dtype=float)/255
X_test = np.asarray(X_test, dtype=float)/255
y_train = y_train.flatten()
y_test = y_test.flatten()

X_test.shape

print("Number of train images: ", X_train.shape[0])
print("Number of test images: ", X_test.shape[0])

from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier
parameter_space = {
    'hidden_layer_sizes': [(50,50,50), (100,)],
    'activation': ['tanh', 'relu'],
    'alpha': [0.0001, 0.05]
}
clf = MLPClassifier(max_iter = 300,random_state=42)
# clf.fit(X_train,y_train.values.ravel())
clf_GS_mlp = GridSearchCV(clf, parameter_space, cv=2)
clf_GS_mlp.fit(X_train, y_train)
clf_GS_mlp.best_params_

from sklearn.model_selection import cross_val_score 
model_f= MLPClassifier(max_iter = 300,random_state=42)
cv_results_f = cross_val_score(model, X_train, y_train,cv = 5, scoring='accuracy', n_jobs = -1, verbose = 1)  
model_f.fit(X_train, y_train)

from sklearn import metrics
print(cv_results_f.mean()*100)
predicted_y_f = model.predict(X_test)
print(metrics.classification_report(y_test, predicted_y_f))